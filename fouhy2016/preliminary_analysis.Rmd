---
title: "Preliminary DADA2 analysis of the Fouhy2016 dataset"
---

# Notes

[Note about using dada2 with Ion Torrent data](https://benjjneb.github.io/dada2/faq.html#can-i-use-dada2-with-my-454-or-ion-torrent-data)

PGM reads are single end; MiSeq reads are paired end. Since I don't know how to
split the MiSeq reads, let's just work with the PGM reads.

# Starting point

Reads are stored in `DATA_PATH/fouhy2016/reads` in compressed fastq files named
by the SRA run accession. 

Next, I need to check that all non-biological nucleotides have been removed,
e.g. primers.

I need to pick filter and trim parameters.

The lengths of the reads are highly variable and so I'll need to look at the
length distribution and decide what length to filter below (`maxLen`).

# Getting ready

Load packages.
```{r libraries}
library(dada2); packageVersion("dada2")
library(ShortRead); packageVersion("ShortRead")
library(Biostrings); packageVersion("Biostrings")

library(tidyverse)
```
Set paths.
```{r path}
dotenv::load_dot_env("../.env")
script_path <- getwd()
data_path <- file.path(Sys.getenv("DATA_DIR"),
    "fouhy2016")
tax_path <- Sys.getenv("TAX_DB")
species_path <- Sys.getenv("SPECIES_DB")
reads_path <- file.path(data_path, "reads")
list.files(reads_path) %>% length
list.files(reads_path) %>% head
```
Load the sample metadata.
```{r}
sam <- readr::read_csv(file.path(script_path, "sample_metadata.csv"))
```
We'll just be working with the PGM samples with the V45 primer set for now.
```{r}
sam0 <- sam %>%
    filter(Sequencer == "PGM", Primer == "V45")
sam0
```
Get the sequence files.
```{r filenames}
fns <- file.path(reads_path, paste0(sam0$SRA_run, ".fastq.gz"))
names(fns) <- sam0$SRA_run
all(fns %in% list.files(reads_path, pattern=".fastq.gz", full.names = TRUE))
```

Note, we will probably want to separately choose trim parameters for the
different primer sets (or at least the different primer regions). Not sure
about learning the errors.

From the Methods: "PGM: V1-V2 primers (335-355 bp), and V4-V5 primer (380-400
bp)".


```{r load_primers}
primers <- readr::read_csv(file.path(script_path, "primers.csv"))
print(primers)
primers.v45 <- primers %>%
    filter(Primer_set == "V45") %>%
    select(Direction, Spacer, Primer_sequence) %>%
    unique
print(primers.v45)
primer_pair <- primers.v45 %>%
    mutate(Sequence = paste0(Spacer, Primer_sequence)) %>%
    {split(.$Sequence, .$Direction)}
FWD <- primer_pair$Forward
REV <- primer_pair$Reverse
```


# Check for primers.

```{r}
allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
    orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
        RevComp = reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
}
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients
```

TODO: only run if files not yet existing.
```{r, eval = F}
# Put N-filterd files in filtN/ subdirectory
fns.filtN <- file.path(reads_path, "filtN", basename(fns))

out <- filterAndTrim(fns, fns.filtN, maxN = 0, multithread = TRUE)

primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}
rbind(FWD = sapply(FWD.orients, primerHits, fn = fns.filtN[[1]]), 
    REV = sapply(REV.orients, primerHits, fn = fns.filtN[[1]]))
```

Remove primers with cutadapt.
```{r}
system2("cutadapt", args = "--version")

path.cut <- file.path(reads_path, "cutadapt")
if(!dir.exists(path.cut)) dir.create(path.cut)
fns.cut <- file.path(path.cut, basename(fns))
names(fns.cut) <- names(fns)

FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)
# Trim FWD and the reverse-complement of REV off of reads
# flags <- paste("-g", paste0("^", FWD), "-a", REV.RC) 
flags <- paste("-a", paste0(FWD, "...", REV.RC))
# Run Cutadapt
report_fn <- file.path(path.cut, "cutadapt_report.txt")
for (i in seq_along(fns)) {
    command <- paste("cutadapt", flags, "-o", fns.cut[i], fns.filtN[i],
        "--minimum-length 1", 
        "| tee -a", report_fn)
    system(command)
}

# Check that primers are (mostly) gone
rbind(FWD = sapply(FWD.orients, primerHits, fn = fns.cut[[1]]), 
    REV = sapply(REV.orients, primerHits, fn = fns.cut[[1]]))
```


# Choosing filter and trim parameters

## Read length distribution

```{r}
dna <- readDNAStringSet(fns.cut[1], format = "fastq")
qplot(dna@ranges@width, log="y")
dna@ranges@width %>%
    summary
```

TODO: figure out how to verify the target length after primers removed.

## Inspect read quality profiles

We start by visualizing the quality profiles of the forward reads:
```{r see_quality}
qps <- plotQualityProfile(fns)
qps + 
    geom_vline(xintercept = 300) +
    geom_vline(xintercept = 365)
```

From the DADA2 tutorial:

> In gray-scale is a heat map of the frequency of each quality score at each base
> position. The median quality score at each position is shown by the green line,
> and the quartiles of the quality score distribution by the orange lines. The
> red line shows the scaled proportion of reads that extend to at least that
> position (this is more useful for other sequencing technologies, as Illumina
> reads are typically all the same length, hence the flat red line).

# Filter and trim

Since our goal is just to be able to distinguish the mock strains, we can
truncate a bit to keep more reads. I may want to revisit this after seeing if
we can assign our SVs to the mock strains.
```{r filter, message=FALSE, warning=FALSE}
# Place filtered files in filtered/ subdirectory
filts <- file.path(reads_path, "filtered", 
    paste0(names(fns), "_filt.fastq.gz"))
out <- filterAndTrim(fns, filts, truncLen = 300,
              maxN = 0, maxEE = 2, truncQ = 2, rm.phix = TRUE,
              compress = TRUE, multithread = TRUE)
head(out)
```

# Learn the Error Rates

```{r learn_errors}
err <- learnErrors(filts, multithread = TRUE)
```

Check the estimated error rates:
```{r plot_errors, warning=FALSE}
plotErrors(err, nominalQ = TRUE)
```

Poor fit at high Q may be because there are few bases with such high Q.

# Dereplication

```{r dereplicate, message=FALSE}
dereps <- derepFastq(filts, verbose=TRUE)
# Name the derep-class objects by the sample names
names(dereps) <- names(fns)
```

# Sample Inference

```{r dada}
dada_obj <- dada(dereps, err = err, multithread = TRUE)
```
Inspecting the returned `dada-class` object:
```{r see_dada}
dada_obj[[1]]
```

# Merge paired reads

Only needed for the MiSeq reads.

# Construct sequence table

```{r seqtab}
seqtab <- makeSequenceTable(dada_obj)
dim(seqtab)
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
```
All sequences have the same length, since we are using truncated single-end
reads.

# Remove chimeras

```{r chimeras, message=FALSE}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus",
    multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
ncol(seqtab.nochim)/ncol(seqtab)
sum(seqtab.nochim)/sum(seqtab)
```

TODO: consider how chimera removal differs between single and paired end reads.

# Track reads through the pipeline

Check number of reads that made it through each step in the pipeline:
```{r track}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dada_obj, getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoised", "nonchim")
rownames(track) <- names(fns)
track
```

# Assign taxonomy

TODO: print the tax db file names to show the silva versions.

```{r taxify}
tax <- assignTaxonomy(seqtab.nochim, tax_path, multithread=TRUE)
```

```{r species}
tax <- addSpecies(tax, species_path)
```

Inspect the taxonomic assignments:
```{r see_tax}
tax %>%
    as_tibble %>%
    select(Genus:Species) %>%
    print(n=30)
```


```{r}
s <- tax %>%
    as_tibble(rownames = "Sequence") %>%
    filter(Genus == "Pseudomonas") %>%
    {.$Sequence}

s
# [1] "TACGAAGGGTGCAAGCGTTAATCGGAATTACTGGGCGTAAAGCGCGCGTAGGTGGTTCAGCAAGTTGGATGTGAAATCCCCGGGCTCAACCTGGGAACTGCATCCAAAACTACTGAGCTAGAGTACGGTAGAGGGTGGTGGAATTTCCTGTGTAGCGGTGAAATGCGTAGATATAGGAAGGAACACCAGTGGCGAAGGCGACCACCTGGACTGATACTGACACTGAGGTGCGAAAGCGTGGGGAGCAAACAGGATTAGATACCCTGGTAGTCCACGCCGTAAACGATGTCGACTAGCCGTTGGGATCCT"
```

Ask Ben what the best way to get accurate calls on the mock is.


Then try improving by using cutadapt and playing with trunc param.

Then generalize to a pipeline for all primer sets.

Save all these objects.
```{r}
list(err, track, seqtab, seqtab.nochim, tax) %>%
    walk(~saveRDS(., file.path(data_path, "dada_out",
                deparse(substitute(.)))) )

list.files(file.path(data_path, "dada_out"))

list(err, track, seqtab, seqtab.nochim, tax) %>%
    map(function (x) print(file.path(data_path, "dada_out",
                deparse(substitute(x)))) )


li <- list(err, track, seqtab, seqtab.nochim, tax) %>%
    map_chr(my_save)
for (i in seq_along(li)) {
    print(file.path(data_path, "dada_out", deparse(substitute(li[[i]]))))
    # walk(~saveRDS(., file.path(data_path, "dada_out",
    #             deparse(substitute(.)))) )
}

#### This works, I wish the other tries would
my_save <- function(obj) {
    bn <- deparse(substitute(obj))
    fn <- file.path(data_path, "dada_out", paste0(bn, ".Rds"))
    saveRDS(obj, fn)
}
my_save(err)
my_save(track)
my_save(seqtab)
my_save(seqtab.nochim)
my_save(tax)

list.files(file.path(data_path, "dada_out"))
```
